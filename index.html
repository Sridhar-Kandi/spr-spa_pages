<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML Concepts: GMM, EM, K-Means</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;600;700;800&display=swap" rel="stylesheet">
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js"></script>
    <!-- Visualization & Content Choices:
        - Introduction: Welcoming text.
        - Core Concepts (GMM): Chart.js scatter for unimodal/multimodal comparison, Chart.js for GMM fit. Goal: Illustrate data distribution and GMM's solution.
        - Challenges: Chart.js line plots for convex/non-convex functions. Goal: Explain optimization landscape issues.
        - EM Algorithm: Conceptual Chart.js animation of Gaussian components adjusting. Goal: Demystify EM iterations.
        - K-Means Clustering: Chart.js scatter with dynamic centroid and assignment updates. Goal: Show K-Means iterative process.
        - K-Means Discussion: Textual explanation of complexity, convergence, and model selection.
        - Deeper Dive EM: Detailed explanation of EM steps, formulas, derivations, and probability foundations.
        - Formulas: Categorized formulas for GMM, K-Means, and EM components.
        - Summary: Consolidated takeaways.
        - All textual content is adapted for clarity and includes intuition and examples.
        - CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body { font-family: 'Nunito', sans-serif; }
        .chart-container { position: relative; width: 100%; max-width: 800px; /* Increased max-width for better visual */ margin-left: auto; margin-right: auto; height: 350px; max-height: 450px; }
        @media (min-width: 768px) { .chart-container { height: 400px; } } /* Adjusted height for larger screens */
        .tooltip { position: relative; display: inline-block; cursor: pointer; }
        .tooltip .tooltiptext {
            visibility: hidden; width: 220px; background-color: #333; color: #fff; text-align: center;
            border-radius: 6px; padding: 5px 0; position: absolute; z-index: 10; bottom: 125%; left: 50%;
            margin-left: -110px; opacity: 0; transition: opacity 0.3s; font-size: 0.875rem;
            line-height: 1.4;
        }
        .tooltip:hover .tooltiptext { visibility: visible; opacity: 1; }
        .formula-term { border-bottom: 1px dotted #6366f1; font-weight: bold; } /* indigo-500 */
        .sticky-sidebar { position: sticky; top: 0; height: 100vh; overflow-y: auto; }
        section { scroll-margin-top: 20px; } /* Offset for sticky nav if any, or general scroll alignment */
    </style>
</head>
<body class="bg-neutral-50 text-neutral-800 antialiased flex">

    <nav class="sticky-sidebar w-64 bg-neutral-100 shadow-lg p-6 hidden md:block">
        <h2 class="text-2xl font-bold text-blue-800 mb-6">Concepts</h2>
        <ul class="space-y-3">
            <li><a href="#intro" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">Introduction</a></li>
            <li><a href="#gmm-concepts" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">Gaussian Mixture Models</a></li>
            <li><a href="#challenges" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">Learning Challenges</a></li>
            <li><a href="#em-algo" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">EM Algorithm</a></li>
            <li><a href="#kmeans-algo" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">K-Means Clustering</a></li>
            <li><a href="#kmeans-discussion" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">K-Means Discussion</a></li>
            <li><a href="#em-deep-dive" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">Deeper Dive into EM</a></li>
            <li><a href="#formulas" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">Key Formulas</a></li>
            <li><a href="#summary" class="block text-neutral-700 hover:text-indigo-600 font-medium py-2 px-3 rounded-md transition duration-150 ease-in-out">Summary</a></li>
        </ul>
    </nav>

    <main class="flex-1 p-4 md:p-8 overflow-y-auto">

        <section id="intro" class="my-8 p-6 bg-white rounded-xl shadow-md">
            <h1 class="text-4xl font-extrabold text-blue-800 mb-6 text-center md:text-left">Machine Learning Concepts: GMM, EM, & K-Means</h1>
            <p class="text-lg text-neutral-700 leading-relaxed">
                Welcome to this interactive guide! We'll explore fundamental machine learning concepts: Gaussian Mixture Models (GMMs), the Expectation-Maximization (EM) algorithm, and K-Means Clustering. Our goal is to demystify these powerful tools, making them accessible with clear explanations, relatable examples, and engaging visuals.
            </p>
        </section>

        <section id="gmm-concepts" class="my-12 p-6 bg-white rounded-xl shadow-md">
            <h2 class="text-3xl font-bold text-blue-800 mb-6">Gaussian Mixture Models (GMMs)</h2>
            <p class="mb-6 text-neutral-700 leading-relaxed">
                Imagine you're trying to understand how a group of data points is distributed. Simple models, like a single bell curve (a "unimodal" distribution), work well for data that clusters around one average. But what if your data has multiple distinct groups, like heights of men and women? That's a "multimodal" distribution, and a single bell curve just won't cut it.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4">The Problem: Unimodal vs. Multimodal Data</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                A single distribution model struggles to represent data that naturally forms several clusters. The chart below shows data points from two distinct groups. A single, large orange curve tries to fit them all, but it's a poor fit, stretching to cover both without accurately capturing their individual densities.
            </p>
            <div class="chart-container bg-neutral-50 p-4 rounded-lg border border-neutral-200">
                <canvas id="unimodalVsMultimodalChart"></canvas>
            </div>
            <p class="mt-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: Trying to fit a single average to two very different groups (like average height for all adults, ignoring gender) makes the average less meaningful for either group.
            </p>
            <p class="mt-4 text-neutral-700 leading-relaxed">
                <strong>Example</strong>: If you average the heights of 5-year-olds and 40-year-olds together, the single average won't accurately describe either age group.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">The Solution: Mixing Gaussians</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                The idea behind GMMs is to combine several simpler, unimodal Gaussian distributions. Each Gaussian in the mixture can then model a different cluster or "mode" in your data. It's like mixing different colors of paint to get a complex shade.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                The chart below demonstrates how two separate Gaussians (blue and green ellipses) can now effectively model the same two-cluster dataset. The combined GMM (represented by both ellipses) accurately captures the distinct groups, providing a much better fit.
            </p>
            <div class="chart-container bg-neutral-50 p-4 rounded-lg border border-neutral-200">
                <canvas id="gmmFitChart"></canvas>
            </div>
            <p class="mt-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: Instead of one "average" for everyone, we find an average height for men and an average height for women. Then, we combine these two averages, considering how many men and women are in the population.
            </p>
            <p class="mt-4 text-neutral-700 leading-relaxed">
                <strong>Example</strong>: To model adult heights, a GMM might use one Gaussian centered at average male height and another at average female height. Each Gaussian would have its own spread, and they'd be weighted by the proportion of men and women in the population.
            </p>
        </section>

        <section id="challenges" class="my-12 p-6 bg-white rounded-xl shadow-md">
            <h2 class="text-3xl font-bold text-blue-800 mb-6">Challenges in Learning GMMs</h2>
            <p class="mb-6 text-neutral-700 leading-relaxed">
                While GMMs are powerful, figuring out their best parameters (the means, shapes, and weights of each Gaussian) from data isn't straightforward. We face a few key challenges:
            </p>
            <ul class="space-y-4 mb-8 text-neutral-700 list-disc pl-5">
                <li>
                    <strong class="text-indigo-600">Latent Variables:</strong> We don't know which Gaussian component generated each data point. This "hidden" information (like whether a person's height came from the "male" or "female" group) is called a <strong class="text-indigo-600">latent variable</strong>. If we knew these assignments, learning the GMM parameters would be easy.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: You see a height, but you don't know the gender. The gender is hidden, but crucial for understanding the height distribution.</p>
                </li>
                <li>
                    <strong class="text-indigo-600">Mutual Dependence:</strong> The assignments of points to Gaussians depend on the Gaussian parameters, and the parameters depend on the assignments. It's a "chicken-and-egg" problem where you need one to find the other.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: To know which cluster a point belongs to, you need the cluster centers. To find the cluster centers, you need to know which points belong to them.</p>
                </li>
                <li>
                    <strong class="text-indigo-600">No Closed-Form Solution for MLE:</strong> We can't just write a direct mathematical formula (a <strong class="text-indigo-600">closed-form solution</strong>) to find the best parameters using Maximum Likelihood Estimation (MLE). The equations become too complex to solve directly.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: You can't just plug numbers into a simple equation to get the answer. You need a more involved, step-by-step process.</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: Solving $2x=4$ gives $x=2$ (closed-form). Finding the best GMM parameters is like trying to solve an equation where $x$ is on both sides in a very complicated way, requiring an iterative approach.</p>
                </li>
                <li>
                    <strong class="text-indigo-600">Non-Convex Likelihood Function:</strong> The function we try to maximize (the likelihood function) is "non-convex." This means its shape is complex, with many "dips" or local optima, not just one clear peak.
                </li>
            </ul>
            <div class="grid md:grid-cols-2 gap-8 items-start mt-8">
                <div class="bg-neutral-50 p-4 rounded-lg border border-neutral-200">
                    <h3 class="text-xl font-semibold text-blue-700 mb-2 text-center">Convex Function</h3>
                    <p class="text-sm text-neutral-600 mb-2 text-center">Like a simple bowl. Any local minimum is the global minimum. Easy to optimize.</p>
                    <div class="chart-container h-64">
                        <canvas id="convexChart"></canvas>
                    </div>
                    <p class="mt-2 text-sm text-neutral-700 leading-relaxed">
                        <strong>Intuition</strong>: Imagine a perfectly smooth valley. No matter where you start, if you always walk downhill, you'll eventually reach the very bottom (the global minimum).
                    </p>
                    <p class="mt-2 text-sm text-neutral-600 leading-relaxed">
                        <strong>Example</strong>: The cost function for simple linear regression is convex. This is why gradient descent always finds the optimal solution.
                    </p>
                </div>
                <div class="bg-neutral-50 p-4 rounded-lg border border-neutral-200">
                    <h3 class="text-xl font-semibold text-blue-700 mb-2 text-center">Non-Convex Function</h3>
                    <p class="text-sm text-neutral-600 mb-2 text-center">Wavy, with multiple "dips" (local minima). Optimization can get stuck.</p>
                    <div class="chart-container h-64">
                        <canvas id="nonConvexChart"></canvas>
                    </div>
                    <p class="mt-2 text-sm text-neutral-700 leading-relaxed">
                        <strong>Intuition</strong>: Imagine a mountain range with many valleys. If you always walk downhill, you'll reach the bottom of *some* valley (a local minimum), but it might not be the deepest valley in the entire range (the global minimum).
                    </p>
                    <p class="mt-2 text-sm text-neutral-600 leading-relaxed">
                        <strong>Example</strong>: The likelihood function for GMMs is non-convex. This means an algorithm might find a good set of parameters, but not necessarily the absolute best possible set.
                    </p>
                </div>
            </div>
        </section>

        <section id="em-algo" class="my-12 p-6 bg-white rounded-xl shadow-md">
            <h2 class="text-3xl font-bold text-blue-800 mb-6">The Expectation-Maximization (EM) Algorithm</h2>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                Since we can't find a direct solution for GMMs, we use an iterative approach called the <strong class="text-indigo-600">Expectation-Maximization (EM) Algorithm</strong>. It's a clever way to handle the "chicken-and-egg" problem by taking turns refining our guesses.
            </p>
            <ol class="space-y-4 mb-6 text-neutral-700 list-decimal pl-5">
                <li>
                    <strong class="text-emerald-600">E-Step (Expectation):</strong> Given our *current best guess* for the GMM parameters, we calculate the "responsibility" of each Gaussian component for each data point. This means, for every point, we figure out the *probability* that it belongs to each of the $K$ Gaussians. This is like making "soft assignments."
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: "If these were our cluster centers, how strongly does each data point belong to each cluster?"</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: If a person is 170cm tall, and we have a "male" Gaussian (mean 175cm) and "female" Gaussian (mean 160cm), the E-step would tell us the probability that this 170cm person belongs to the male group vs. the female group, based on the current Gaussian parameters.</p>
                </li>
                <li>
                    <strong class="text-emerald-600">M-Step (Maximization):</strong> Using these "soft assignments" (probabilities) from the E-step, we then **re-estimate the GMM parameters** (means, covariances, and mixing weights) to best fit the data points according to their assigned responsibilities. We treat these probabilities as if they were fixed.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: "Now that we have a better idea of which points belong where, let's adjust our cluster centers and shapes to best fit these probabilistic assignments."</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: If many 170cm people now have a high probability of being "male," the "male" Gaussian's mean might shift slightly towards 170cm, and its spread might adjust to better cover these points.</p>
                </li>
            </ol>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                The visualization below demonstrates the EM algorithm conceptually. Click "Next EM Step" to see how the Gaussians (ellipses) and data point "assignments" (colors) adjust over iterations, gradually converging to a better fit.
            </p>
            <div class="chart-container bg-neutral-50 p-4 rounded-lg border border-neutral-200">
                <canvas id="emAlgorithmChart"></canvas>
            </div>
            <div class="text-center mt-6">
                <button id="emNextStep" class="bg-indigo-600 hover:bg-indigo-700 text-white font-semibold py-2 px-6 rounded-lg shadow transition duration-150 ease-in-out">
                    Next EM Step (Conceptual)
                </button>
                <button id="emReset" class="ml-4 bg-neutral-500 hover:bg-neutral-600 text-white font-semibold py-2 px-6 rounded-lg shadow transition duration-150 ease-in-out">
                    Reset Animation
                </button>
                 <p id="emIterationText" class="mt-2 text-sm text-neutral-600">Iteration: 0</p>
            </div>
            <p class="mt-6 text-neutral-700 leading-relaxed">
                EM is guaranteed to improve the likelihood of the data with each step, converging to a local maximum. While not always the absolute global best, it's a very effective solution in practice.
            </p>
        </section>

        <section id="kmeans-algo" class="my-12 p-6 bg-white rounded-xl shadow-md">
            <h2 class="text-3xl font-bold text-blue-800 mb-6">K-Means Clustering: An "EM-Light" Algorithm</h2>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                K-Means is a simpler, yet very popular, clustering algorithm that can be seen as a special, "harder" version of the EM algorithm. Instead of probabilistic assignments, K-Means makes definite assignments: each point belongs to exactly one cluster.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4">What K-Means Does: Given and Sought</h3>
            <ul class="space-y-3 mb-6 text-neutral-700 list-disc pl-5">
                <li>
                    <strong class="text-indigo-600">Given:</strong> A set of data points X = {x<sub>1</sub>, ..., x<sub>N</sub>}.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: This is your raw data, like a list of customer details (age, income, spending).</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: You have data for 100 customers (N=100), where each x<sub>n</sub> is a customer's (Age, Income).</p>
                </li>
                <li>
                    <strong class="text-indigo-600">Sought:</strong>
                    <ol class="list-decimal list-inside pl-4 mt-1">
                        <li>Assignment of each data point to one of K clusters.
                            <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: We want to divide our customers into K distinct groups, where each customer belongs to only one group.</p>
                            <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: Grouping 100 customers into K=3 segments (e.g., "Young Savers," "Mid-Age Spenders," "Elderly Investors").</p>
                        </li>
                        <li>The cluster centroids &mu;<sub>k</sub>.
                            <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: For each group, we find a central point that best represents it. This "centroid" is like the average customer profile for that group.</p>
                            <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: The centroid for "Young Savers" might be (25 years, $30,000 income$).</p>
                        </li>
                    </ol>
                </li>
            </ul>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4">The K-Means Objective (Cost) Function</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                K-Means tries to minimize the total squared distance between each data point and the center of the cluster it's assigned to. This makes the clusters as "tight" as possible.
            </p>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    J = &Sigma;<sub>n=1</sub><sup>N</sup> &Sigma;<sub>k=1</sub><sup>K</sup>
                    <span class="tooltip formula-term"> r<sub>nk</sub> <span class="tooltiptext">Binary assignment: 1 if data point n is in cluster k, 0 otherwise. (Hard assignment)</span></span>
                    <span class="tooltip formula-term"> ||x<sub>n</sub> - &mu;<sub>k</sub>||<sup>2</sup> <span class="tooltiptext">Squared Euclidean distance between data point x<sub>n</sub> and cluster centroid &mu;<sub>k</sub>.</span></span>
                </p>
                <ul class="mt-4 text-sm text-neutral-600 list-disc list-inside">
                    <li><span class="font-semibold">J:</span> The objective function to minimize (total squared distance).</li>
                    <li><span class="font-semibold">N:</span> Total number of data points.</li>
                    <li><span class="font-semibold">K:</span> Total number of clusters.</li>
                </ul>
            </div>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: Imagine you have a rubber band from each data point to its assigned cluster center. K-Means tries to shrink all these rubber bands as much as possible.
            </p>
            <p class="mb-6 text-neutral-700 leading-relaxed">
                This objective function is based on Euclidean distance and is equivalent to maximizing the likelihood of a very specific, simplified Gaussian Mixture Model (where all Gaussians are perfectly spherical, have the same size, and each point belongs to only one Gaussian).
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4">The K-Means Algorithm Steps (Iterative Process)</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                Like EM, K-Means is an iterative algorithm. It alternates between two steps, refining its clusters until they stabilize. These steps are direct parallels to the E-step and M-step of the general EM algorithm:
            </p>
            <ol class="space-y-4 mb-6 text-neutral-700 list-decimal pl-5">
                <li>
                    <strong class="text-emerald-600">Initialization:</strong> Start by picking $K$ random points from your data as initial cluster centroids.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: Just like starting a game, you need to place your pieces on the board first.</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: Randomly select 3 customers from your 100 as the initial "average" profiles for your 3 segments.</p>
                </li>
                <li>
                    <strong class="text-emerald-600">E-Step (Assignment Step):</strong> For each data point, assign it to the cluster whose centroid is closest.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: "Every data point goes to its nearest neighbor cluster."</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: Each of your 100 customers is assigned to the "average customer profile" (centroid) that they are most similar to (closest in terms of age and income).</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Formal Rule</strong>: r<sub>nk</sub> = 1 if k = argmin<sub>j</sub> ||x<sub>n</sub> - &mu;<sub>j</sub>||<sup>2</sup>, else 0.</p>
                </li>
                <li>
                    <strong class="text-emerald-600">M-Step (Update Step):</strong> For each cluster, update its centroid to be the mean (average position) of all data points currently assigned to that cluster.
                    <p class="text-sm text-neutral-600 mt-1"><strong>Intuition</strong>: "Now that we know who belongs to each cluster, let's move the cluster center to the true middle of its members."</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Example</strong>: The "Young Savers" centroid is updated to be the average age and income of all customers who were assigned to that segment in the previous step.</p>
                    <p class="text-sm text-neutral-600 mt-1"><strong>Formal Rule</strong>: &mu;<sub>k</sub> = &Sigma;<sub>n</sub> r<sub>nk</sub> x<sub>n</sub> / &Sigma;<sub>n</sub> r<sub>nk</sub></p>
                </li>
            </ol>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                These two steps are repeated until the cluster assignments no longer change, or the centroids stop moving significantly.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">K-Means Step-by-Step Visualization</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                Watch the K-Means algorithm in action below. Click "Next K-Means Step" to see how the algorithm iteratively refines its clusters.
            </p>
            <div class="chart-container bg-neutral-50 p-4 rounded-lg border border-neutral-200">
                <canvas id="kmeansAlgorithmChart"></canvas>
            </div>
            <div class="text-center mt-6">
                <button id="kmeansNextStep" class="bg-indigo-600 hover:bg-indigo-700 text-white font-semibold py-2 px-6 rounded-lg shadow transition duration-150 ease-in-out">
                    Next K-Means Step
                </button>
                <button id="kmeansReset" class="ml-4 bg-neutral-500 hover:bg-neutral-600 text-white font-semibold py-2 px-6 rounded-lg shadow transition duration-150 ease-in-out">
                    Reset Animation
                </button>
                 <p id="kmeansIterationText" class="mt-2 text-sm text-neutral-600">Iteration: 0</p>
            </div>
            <p class="mt-6 text-neutral-700 leading-relaxed">
                <strong>Overall Intuition</strong>: The algorithm is like a game of "musical chairs" for data points. Points move to the closest chair (centroid), then the chairs move to the center of their new groups. This repeats until no one wants to move chairs anymore.
            </p>
        </section>

        <section id="kmeans-discussion" class="my-12 p-6 bg-white rounded-xl shadow-md">
            <h2 class="text-3xl font-bold text-blue-800 mb-6">K-Means Discussion & Considerations</h2>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                This section covers important practical considerations and theoretical properties of the K-Means clustering algorithm.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4">Computational Speed ($O(KN)$):</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                K-Means is generally considered a very fast algorithm. Its average computational complexity is **O(KN)**, where $K$ is the number of clusters and $N$ is the number of data points. This means the time it takes to run increases linearly with the number of data points and the number of clusters.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: Imagine you have $N$ students and $K$ classrooms. In each step, you briefly look at each student ($N$) and check which of the $K$ classrooms is closest. This process is very efficient.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Example</strong>: If you double the number of students, K-Means will take roughly twice as long. If you double the number of classrooms, it will also take roughly twice as long.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">Faster Implementations:</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                There are optimized versions (like Elkan's algorithm) that speed up K-Means by avoiding unnecessary distance calculations. They use mathematical shortcuts (like the triangle inequality) to skip calculating distances to centroids that are clearly not the closest.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: Instead of checking every single classroom for every student, you might quickly rule out classrooms that are far away without a full check, saving time.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">Convergence: Local vs. Global Minimum</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                With the standard "full E-step" (where every point is assigned to its closest centroid), K-Means is **guaranteed to converge** to a stable state where further iterations don't change the clusters. However, this stable state is a **local minimum**, not necessarily the **global minimum**.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: Imagine descending into a valley in a mountain range. You'll always reach *a* bottom (a local minimum), but it might not be the *deepest* valley (the global minimum) in the entire range. The starting point (initial centroids) can influence which valley you end up in.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                Conversely, if a "naïve E-step" (less rigorous assignment) is used, K-Means is **not assured to converge**.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Example</strong>: If K-Means starts with centroids in a suboptimal configuration, it might converge to a clustering that is "good enough" but not the absolute best possible grouping of the data. Running K-Means multiple times with different random initializations can help find a better local minimum.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">Model Selection: Choosing K</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                A crucial practical challenge is deciding the optimal number of clusters, $K$. There's no single "correct" answer, and choosing $K$ often involves domain knowledge, visual inspection, or statistical methods (like the elbow method or silhouette score).
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: How many distinct groups *should* there be in your data? Is it 2, 3, or 10? This isn't always obvious just by looking at the data.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Example</strong>: For customer segmentation, should you have 3 customer types or 5? The choice of $K$ dramatically changes the interpretation of the clusters.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">Extreme Case ($K=N$):</h3>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                If you choose $K$ to be equal to $N$ (the number of data points), each data point becomes its own cluster. This extreme case leads to concepts like **Kernel Density Estimators** and **non-parametric methods** (often discussed in more advanced courses).
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Intuition</strong>: If every student gets their own classroom, there's no "clustering" happening in the traditional sense. Each point is treated individually.
            </p>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                <strong>Example</strong>: Instead of grouping people into "tall," "medium," and "short," you describe the probability of *each specific height* occurring, without forcing them into predefined buckets.
            </p>
        </section>

        <section id="formulas" class="my-12 p-6 bg-white rounded-xl shadow-md">
            <h2 class="text-3xl font-bold text-blue-800 mb-6">Key Formulas</h2>
            <p class="mb-4 text-neutral-700 leading-relaxed">
                Here are the core mathematical expressions for the models and algorithms we've discussed, categorized for easier reference. Hover over the bold terms for detailed explanations.
            </p>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4">Gaussian Mixture Model (GMM) Formulas:</h3>
            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">GMM Probability Density Function:</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    p(x) = &Sigma;<sub>k=1</sub><sup>K</sup>
                    <span class="tooltip formula-term"> &pi;<sub>k</sub> <span class="tooltiptext">Mixture coefficient for component k: the weight or proportion of this Gaussian in the mix. All &pi;<sub>k</sub> sum to 1.</span></span>
                    <span class="tooltip formula-term"> &#8475;(x | &mu;<sub>k</sub>, &Sigma;<sub>k</sub>) <span class="tooltiptext">The k-th Gaussian component, defined by its mean (&mu;<sub>k</sub>) and covariance (&Sigma;<sub>k</sub>). It describes the probability of x given this component.</span></span>
                </p>
            </div>

            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">Log-Likelihood Function for GMM:</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    ln p(X|&pi;, &mu;, &Sigma;) = &Sigma;<sub>n=1</sub><sup>N</sup> ln ( &Sigma;<sub>k=1</sub><sup>K</sup> &pi;<sub>k</sub> &#8475;(x<sub>n</sub>|&mu;<sub>k</sub>, &Sigma;<sub>k</sub>) )
                </p>
            </div>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">Expectation-Maximization (EM) Algorithm Formulas:</h3>
            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">E-Step: Responsibility (Posterior Probability) Formula:</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    &gamma;(z<sub>nk</sub>) = p(z<sub>k</sub> = 1|x<sub>n</sub>) = <br>
                    <span class="inline-block align-middle">&frac;&#8475;(x<sub>n</sub>|&mu;<sub>k</sub>,&Sigma;<sub>k</sub>)&pi;<sub>k</sub>;</span><span class="inline-block align-middle">&Sigma;<sub>j=1</sub><sup>K</sup> &#8475;(x<sub>n</sub>|&mu;<sub>j</sub>,&Sigma;<sub>j</sub>)&pi;<sub>j</sub>;</span>
                </p>
            </div>

            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">M-Step: Mean Update Rule:</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    &mu;<sub>k</sub><sup>new</sup> = <span class="inline-block align-middle">&frac;&Sigma;<sub>n=1</sub><sup>N</sup> &gamma;(z<sub>nk</sub>) x<sub>n</sub>;&Sigma;<sub>n=1</sub><sup>N</sup> &gamma;(z<sub>nk</sub>);</span>
                </p>
            </div>

            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">M-Step: Covariance Update Rule:</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    &Sigma;<sub>k</sub><sup>new</sup> = <span class="inline-block align-middle">&frac;&Sigma;<sub>n=1</sub><sup>N</sup> &gamma;(z<sub>nk</sub>) (x<sub>n</sub> - &mu;<sub>k</sub><sup>new</sup>)(x<sub>n</sub> - &mu;<sub>k</sub><sup>new</sup>)<sup>T</sup>;&Sigma;<sub>n=1</sub><sup>N</sup> &gamma;(z<sub>nk</sub>);</span>
                </p>
            </div>

            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">M-Step: Mixture Coefficient Update Rule:</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    &pi;<sub>k</sub><sup>new</sup> = <span class="inline-block align-middle">&frac;&Sigma;<sub>n=1</sub><sup>N</sup> &gamma;(z<sub>nk</sub>);N;</span>
                </p>
            </div>

            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">Law of Total Probability (General Form):</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    P(A) = &Sigma;<sub>i=1</sub><sup>M</sup> P(A | B<sub>i</sub>) P(B<sub>i</sub>)
                </p>
            </div>

            <h3 class="text-2xl font-semibold text-blue-700 mb-4 mt-8">K-Means Clustering Formulas:</h3>
            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">K-Means Objective (Cost) Function:</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    J = &Sigma;<sub>n=1</sub><sup>N</sup> &Sigma;<sub>k=1</sub><sup>K</sup>
                    <span class="tooltip formula-term"> r<sub>nk</sub> <span class="tooltiptext">Binary assignment: 1 if data point n is in cluster k, 0 otherwise. (Hard assignment)</span></span>
                    <span class="tooltip formula-term"> ||x<sub>n</sub> - &mu;<sub>k</sub>||<sup>2</sup> <span class="tooltiptext">Squared Euclidean distance between data point x<sub>n</sub> and cluster centroid &mu;<sub>k</sub>.</span></span>
                </p>
            </div>

            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">K-Means Assignment Rule (E-Step):</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    r<sub>nk</sub> = 1 if k = argmin<sub>j</sub> ||x<sub>n</sub> - &mu;<sub>j</sub>||<sup>2</sup>, else 0.
                </p>
            </div>

            <h4 class="text-xl font-semibold text-blue-600 mb-2 mt-4">K-Means Centroid Update Rule (M-Step):</h4>
            <div class="mb-6 p-4 bg-neutral-100 rounded-lg border border-neutral-200">
                <p class="text-lg text-center font-mono text-neutral-700 break-all">
                    &mu;<sub>k</sub> = &Sigma;<sub>n</sub> r<sub>nk</sub> x<sub>n</sub> / &Sigma;<sub>n</sub> r<sub>nk</sub>
                </p>
            </div>
        </section>

        <section id="summary" class="my-12 p-6 bg-white rounded-xl shadow-md">
            <h2 class="text-3xl font-bold text-blue-800 mb-6">Summary & Key Takeaways</h2>
            <ul class="list-disc list-inside space-y-3 text-neutral-700 leading-relaxed">
                <li>Simple distribution models struggle with <strong class="text-indigo-600">multimodal data</strong> (data with multiple peaks).</li>
                <li><strong class="text-indigo-600">Gaussian Mixture Models (GMMs)</strong> offer a flexible solution by combining multiple Gaussian components, allowing for "soft assignments."</li>
                <li>Learning GMMs involves dealing with <strong class="text-indigo-600">latent variables</strong> (unknown component assignments) and a <strong class="text-indigo-600">non-convex likelihood function</strong>, meaning no direct closed-form solution.</li>
                <li>The <strong class="text-indigo-600">Expectation-Maximization (EM) algorithm</strong> is an iterative method that effectively finds GMM parameters by alternating between an Expectation step (calculating responsibilities) and a Maximization step (re-estimating parameters). EM converges to a <strong class="text-indigo-600">local optimum</strong>.</li>
                <li><strong class="text-indigo-600">K-Means clustering</strong> is a simpler, widely used algorithm that can be viewed as an "EM-light" approach.</li>
                <li>In K-Means, assignments are <strong class="text-indigo-600">hard</strong> (each point belongs to exactly one cluster).</li>
                <li>The K-Means objective function minimizes the sum of squared distances from each point to its assigned cluster centroid.</li>
                <li>K-Means also uses an iterative <strong class="text-indigo-600">E-step</strong> (assign points to closest centroid) and <strong class="text-indigo-600">M-step</strong> (update centroids to mean of assigned points) until convergence.</li>
            </ul>
        </section>
    </main>

<script>
    // Smooth scrolling for vertical navigation
    document.querySelectorAll('nav a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
            e.preventDefault();
            const targetId = this.getAttribute('href');
            const targetElement = document.querySelector(targetId);
            if (targetElement) {
                targetElement.scrollIntoView({
                    behavior: 'smooth'
                });
            }
        });
    });

    // Chart for Unimodal vs Multimodal
    const unimodalData = [];
    const bimodalData1 = [];
    const bimodalData2 = [];

    // Generate two distinct clusters for bimodal data
    for (let i = 0; i < 50; i++) {
        bimodalData1.push({x: Math.random() * 1.5 + 1.5, y: Math.random() * 1.5 + 6.5 }); // Cluster 1
        bimodalData2.push({x: Math.random() * 1.5 + 5, y: Math.random() * 1.5 + 2.5 }); // Cluster 2
    }
    const allBimodalData = [...bimodalData1, ...bimodalData2];

    const unimodalVsMultimodalCtx = document.getElementById('unimodalVsMultimodalChart').getContext('2d');
    new Chart(unimodalVsMultimodalCtx, {
        type: 'scatter',
        data: {
            datasets: [{
                label: 'Data Points',
                data: allBimodalData,
                backgroundColor: 'rgba(34, 197, 94, 0.6)', // emerald-500
                pointRadius: 5,
                pointHoverRadius: 7
            }, {
                type: 'line', // Conceptual single Gaussian fit
                label: 'Poor Single Gaussian Fit',
                data: [{x: 0.5, y: 3}, {x: 3.5, y: 7.5}, {x: 6.5, y: 3}], // Simplified ellipse contour
                borderColor: 'rgba(234, 88, 12, 0.8)', // orange-600
                backgroundColor: 'rgba(234, 88, 12, 0.1)',
                tension: 0.4,
                fill: true,
                borderWidth: 3,
                pointRadius: 0
            }]
        },
        options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
                x: { title: { display: true, text: 'Feature 1' }, min:0, max:8 },
                y: { title: { display: true, text: 'Feature 2' }, min:0, max:9 }
            },
            plugins: {
                tooltip: {
                    callbacks: {
                        label: function(context) {
                            if (context.dataset.label.includes('Gaussian')) return null;
                            return `Data Point: (${context.parsed.x.toFixed(2)}, ${context.parsed.y.toFixed(2)})`;
                        }
                    }
                }
            }
        }
    });

    // Convex Chart
    const convexCtx = document.getElementById('convexChart').getContext('2d');
    new Chart(convexCtx, {
        type: 'line',
        data: {
            labels: Array.from({length: 21}, (_, i) => (i - 10).toString()), // x from -10 to 10
            datasets: [{
                label: 'f(x) = x^2 (Convex)',
                data: Array.from({length: 21}, (_, i) => (i - 10)**2),
                borderColor: 'rgba(22, 163, 74, 0.8)', // emerald-600
                backgroundColor: 'rgba(22, 163, 74, 0.1)',
                tension: 0.1,
                fill: true,
                pointRadius: 0
            }, {
                label: 'Global Minimum',
                data: [{x: '0', y: 0}],
                pointStyle: 'circle',
                pointRadius: 6,
                pointBorderColor: 'rgba(220, 38, 38, 1)', // red-600
                backgroundColor: 'rgba(220, 38, 38, 0.7)',
                showLine: false
            }]
        },
        options: { responsive: true, maintainAspectRatio: false, scales: { y: { beginAtZero: true } } }
    });

    // Non-Convex Chart
    const nonConvexCtx = document.getElementById('nonConvexChart').getContext('2d');
    new Chart(nonConvexCtx, {
        type: 'scatter', /* Changed to scatter type */
        data: {
            // Function: f(x) = x^3 - 3x
            // Critical points: f'(x) = 3x^2 - 3 = 0 => x^2 = 1 => x = -1, 1
            // f(-1) = (-1)^3 - 3*(-1) = -1 + 3 = 2 (Local Maximum)
            // f(1) = (1)^3 - 3*(1) = 1 - 3 = -2 (Local Minimum / Global Minimum in this range)
            datasets: [{
                label: 'f(x) = x³ - 3x (Non-Convex)',
                data: Array.from({length: 201}, (_, i) => { // Increased data points for smoother curve
                    const x = (i - 100) * 0.025; // x from -2.5 to 2.5 with smaller step
                    return { x: x, y: x**3 - 3*x };
                }),
                borderColor: 'rgba(79, 70, 229, 0.8)', // indigo-600
                backgroundColor: 'rgba(79, 70, 229, 0.1)',
                tension: 0.1,
                fill: false, /* Set fill to false for a clean line */
                pointRadius: 0, /* Hide individual points for a smooth line */
                showLine: true /* Explicitly show the line */
            }, {
                label: 'Local Maximum',
                data: [{x: -1, y: 2}],
                pointStyle: 'triangle', // Indicate local max
                pointRadius: 10,
                pointBorderColor: 'rgba(34, 197, 94, 1)', // emerald-500
                backgroundColor: 'rgba(34, 197, 94, 0.7)',
                showLine: false
            }, {
                label: 'Local / Global Minimum',
                data: [{x: 1, y: -2}],
                pointStyle: 'star', // Indicate local/global min
                pointRadius: 10,
                pointBorderColor: 'rgba(251, 191, 36, 1)', // amber-400
                backgroundColor: 'rgba(251, 191, 36, 0.7)',
                showLine: false
            }]
        },
        options: { responsive: true, maintainAspectRatio: false,
            scales: {
                x: { min: -2.5, max: 2.5, title: {display: true, text: 'x'} }, /* Adjusted range to focus on critical points */
                y: { min: -3, max: 3, title: {display: true, text: 'f(x)'} } /* Adjusted range */
            }
        }
    });


    // EM Algorithm Chart (Conceptual Animation)
    const emCtx = document.getElementById('emAlgorithmChart').getContext('2d');
    let emChart;
    let emIteration = 0;
    const maxEmIterations = 5;
    const emDataPoints = [];
    // Pre-defined data points for consistent visualization
    const initialEmPoints = [
        { x: 1.5, y: 1.8 }, { x: 1.2, y: 2.1 }, { x: 1.8, y: 1.5 }, { x: 2.0, y: 2.3 }, { x: 1.0, y: 1.2 },
        { x: 4.5, y: 4.8 }, { x: 4.2, y: 5.1 }, { x: 4.8, y: 4.5 }, { x: 5.0, y: 5.3 }, { x: 4.0, y: 4.2 },
        { x: 1.7, y: 4.0 }, { x: 2.0, y: 3.5 }, { x: 3.0, y: 4.2 }, { x: 3.5, y: 3.0 }, { x: 2.5, y: 2.8 },
        { x: 5.5, y: 1.8 }, { x: 5.2, y: 2.1 }, { x: 5.8, y: 1.5 }, { x: 6.0, y: 2.3 }, { x: 5.0, y: 1.2 },
        { x: 3.0, y: 6.0 }, { x: 3.5, y: 6.5 }, { x: 4.0, y: 7.0 }, { x: 4.5, y: 6.8 }, { x: 5.0, y: 6.5 }
    ];
    initialEmPoints.forEach(p => emDataPoints.push({...p, color: 'rgba(128, 128, 128, 0.5)'}));

    let emGaussians = [
        { mu_x: 1.0, mu_y: 6.0, sigma_x: 0.8, sigma_y: 0.8, color: 'rgba(59, 130, 246, 0.3)', borderColor: 'rgba(59, 130, 246, 1)' }, // Blue
        { mu_x: 6.0, mu_y: 2.0, sigma_x: 0.8, sigma_y: 0.8, color: 'rgba(16, 185, 129, 0.3)', borderColor: 'rgba(16, 185, 129, 1)' }  // Emerald
    ];

    function createEmChart() {
        if (emChart) {
            emChart.destroy();
        }
        emChart = new Chart(emCtx, {
            type: 'scatter',
            data: {
                datasets: [{
                    label: 'Data Points',
                    data: emDataPoints.map(p => ({x: p.x, y: p.y})),
                    pointBackgroundColor: emDataPoints.map(p => p.color),
                    pointRadius: 5,
                    pointHoverRadius: 7
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    x: { min: 0, max: 7, title: {display: true, text: 'Feature X'} },
                    y: { min: 0, max: 7, title: {display: true, text: 'Feature Y'} }
                },
                animation: {
                    duration: 500 // Faster animation for updates
                },
                plugins: {
                    legend: { display: false },
                    tooltip: { enabled: false },
                    // Custom plugin to draw ellipses
                    customEllipseDrawer: {
                        gaussians: emGaussians // Pass gaussians data
                    }
                }
            },
            plugins: [{
                id: 'customEllipseDrawer',
                beforeDraw: (chart, args, options) => {
                    const ctx = chart.ctx;
                    const xAxis = chart.scales.x;
                    const yAxis = chart.scales.y;
                    options.gaussians.forEach(g => {
                        const pixelX = xAxis.getPixelForValue(g.mu_x);
                        const pixelY = yAxis.getPixelForValue(g.mu_y);
                        // Convert sigma to pixel radius (conceptual, not mathematically exact for covariance matrix)
                        const pixelRadiusX = xAxis.getPixelForValue(g.mu_x + g.sigma_x) - pixelX;
                        const pixelRadiusY = yAxis.getPixelForValue(g.mu_y + g.sigma_y) - pixelY;
                        
                        ctx.beginPath();
                        ctx.ellipse(pixelX, pixelY, Math.abs(pixelRadiusX), Math.abs(pixelRadiusY), 0, 0, 2 * Math.PI);
                        ctx.fillStyle = g.color;
                        ctx.fill();
                        ctx.strokeStyle = g.borderColor;
                        ctx.lineWidth = 2;
                        ctx.stroke();

                        // Draw centroid 'x'
                        ctx.strokeStyle = g.borderColor;
                        ctx.lineWidth = 2;
                        const centroidSize = 10;
                        ctx.beginPath();
                        ctx.moveTo(pixelX - centroidSize, pixelY - centroidSize);
                        ctx.lineTo(pixelX + centroidSize, pixelY + centroidSize);
                        ctx.stroke();
                        ctx.beginPath();
                        ctx.moveTo(pixelX + centroidSize, pixelY - centroidSize);
                        ctx.lineTo(pixelX - centroidSize, pixelY + centroidSize);
                        ctx.stroke();
                    });
                }
            }]
        });
    }
    
    function runEmStep() {
        if (emIteration >= maxEmIterations) {
            document.getElementById('emIterationText').textContent = `Converged (Conceptual - Iteration: ${emIteration})`;
            return;
        }

        emIteration++;
        document.getElementById('emIterationText').textContent = `Iteration: ${emIteration}`;

        const newGaussians = JSON.parse(JSON.stringify(emGaussians)); // Deep copy for updates

        // E-Step: Calculate responsibilities for each point to each Gaussian
        const responsibilities = emDataPoints.map(p => {
            const pointResponsibilities = [];
            let totalLikelihood = 0;

            emGaussians.forEach(g => {
                const dx = p.x - g.mu_x;
                const dy = p.y - g.mu_y;
                // Simplified Gaussian PDF-like term (assuming diagonal covariance for simplicity)
                // L = exp(-0.5 * ( (dx^2 / sigma_x^2) + (dy^2 / sigma_y^2) ))
                const likelihood = Math.exp(-0.5 * ((dx * dx / (g.sigma_x * g.sigma_x)) + (dy * dy / (g.sigma_y * g.sigma_y))));
                pointResponsibilities.push(likelihood);
                totalLikelihood += likelihood;
            });

            // Normalize responsibilities for this point
            return pointResponsibilities.map(r => r / totalLikelihood);
        });

        // Update point colors based on responsibilities (blend)
        emDataPoints.forEach((p, pIndex) => {
            const resp1 = responsibilities[pIndex][0];
            const resp2 = responsibilities[pIndex][1];

            // Define base colors for blending (blue and emerald)
            const color1 = [59, 130, 246]; // blue-500
            const color2 = [16, 185, 129]; // emerald-500

            const r = Math.round(color1[0] * resp1 + color2[0] * resp2);
            const g = Math.round(color1[1] * resp1 + color2[1] * resp2);
            const b = Math.round(color1[2] * resp1 + color2[2] * resp2);
            p.color = `rgba(${r}, ${g}, ${b}, 0.8)`; // Slightly higher opacity for clearer blend
        });

        // M-Step: Update gaussian parameters (mu and sigma)
        newGaussians.forEach((g, gIndex) => {
            let totalResp = 0;
            let weightedSumX = 0;
            let weightedSumY = 0;
            
            emDataPoints.forEach((p, pIndex) => {
                const resp = responsibilities[pIndex][gIndex];
                weightedSumX += p.x * resp;
                weightedSumY += p.y * resp;
                totalResp += resp;
            });

            if (totalResp > 0) {
                // Update means
                g.mu_x = weightedSumX / totalResp;
                g.mu_y = weightedSumY / totalResp;

                // Update sigmas (weighted variance)
                let weightedSumDxSq = 0;
                let weightedSumDySq = 0;
                emDataPoints.forEach((p, pIndex) => {
                    const resp = responsibilities[pIndex][gIndex];
                    weightedSumDxSq += resp * (p.x - g.mu_x) * (p.x - g.mu_x);
                    weightedSumDySq += resp * (p.y - g.mu_y) * (p.y - g.mu_y);
                });
                g.sigma_x = Math.sqrt(weightedSumDxSq / totalResp);
                g.sigma_y = Math.sqrt(weightedSumDySq / totalResp);

                // Clamp sigma to reasonable values to prevent collapse/explosion
                g.sigma_x = Math.max(0.2, Math.min(2.0, g.sigma_x));
                g.sigma_y = Math.max(0.2, Math.min(2.0, g.sigma_y));
            }
        });

        emGaussians = newGaussians; // Update global gaussians
        createEmChart(); // Re-create or update chart
    }

    document.getElementById('emNextStep').addEventListener('click', runEmStep);
    document.getElementById('emReset').addEventListener('click', () => {
        emIteration = 0;
        document.getElementById('emIterationText').textContent = `Iteration: 0`;
        emDataPoints.forEach((p, i) => { // Reset points to initial positions and colors
            p.x = initialEmPoints[i].x;
            p.y = initialEmPoints[i].y;
            p.color = 'rgba(128, 128, 128, 0.5)';
        });
        emGaussians = [ // Reset positions
            { mu_x: 1.0, mu_y: 6.0, sigma_x: 0.8, sigma_y: 0.8, color: 'rgba(59, 130, 246, 0.3)', borderColor: 'rgba(59, 130, 246, 1)' },
            { mu_x: 6.0, mu_y: 2.0, sigma_x: 0.8, sigma_y: 0.8, color: 'rgba(16, 185, 129, 0.3)', borderColor: 'rgba(16, 185, 129, 1)' }
        ];
        createEmChart();
    });

    // K-Means Algorithm Chart (Conceptual Animation)
    const kmeansCtx = document.getElementById('kmeansAlgorithmChart').getContext('2d');
    let kmeansChart;
    let kmeansIteration = 0;
    const maxKmeansIterations = 6; // Max iterations for K-Means visualization

    // Pre-defined data points for consistent K-Means visualization
    const initialKmeansPoints = [
        { x: 1.5, y: 1.8 }, { x: 1.2, y: 2.1 }, { x: 1.8, y: 1.5 }, { x: 2.0, y: 2.3 }, { x: 1.0, y: 1.2 },
        { x: 4.5, y: 4.8 }, { x: 4.2, y: 5.1 }, { x: 4.8, y: 4.5 }, { x: 5.0, y: 5.3 }, { x: 4.0, y: 4.2 },
        { x: 1.7, y: 4.0 }, { x: 2.0, y: 3.5 }, { x: 3.0, y: 4.2 }, { x: 3.5, y: 3.0 }, { x: 2.5, y: 2.8 },
        { x: 5.5, y: 1.8 }, { x: 5.2, y: 2.1 }, { x: 5.8, y: 1.5 }, { x: 6.0, y: 2.3 }, { x: 5.0, y: 1.2 },
        { x: 3.0, y: 6.0 }, { x: 3.5, y: 6.5 }, { x: 4.0, y: 7.0 }, { x: 4.5, y: 6.8 }, { x: 5.0, y: 6.5 }
    ].map(p => ({...p, cluster: -1, color: 'rgba(128, 128, 128, 0.5)'})); // -1 for unassigned

    let kmeansCentroids = [
        { x: 1.0, y: 6.0, color: 'rgba(59, 130, 246, 1)' }, // Blue
        { x: 6.0, y: 2.0, color: 'rgba(220, 38, 38, 1)' }   // Red
    ];

    function createKmeansChart() {
        if (kmeansChart) {
            kmeansChart.destroy();
        }
        kmeansChart = new Chart(kmeansCtx, {
            type: 'scatter',
            data: {
                datasets: [{
                    label: 'Data Points',
                    data: initialKmeansPoints.map(p => ({x: p.x, y: p.y})),
                    pointBackgroundColor: initialKmeansPoints.map(p => p.color),
                    pointRadius: 5,
                    pointHoverRadius: 7
                }, {
                    label: 'Centroids',
                    data: kmeansCentroids.map(c => ({x: c.x, y: c.y})),
                    pointBackgroundColor: kmeansCentroids.map(c => c.color),
                    pointStyle: 'crossRot', // 'x' shape
                    pointRadius: 10,
                    pointBorderWidth: 3,
                    pointHoverRadius: 12
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    x: { min: 0, max: 7, title: {display: true, text: 'Feature X'} },
                    y: { min: 0, max: 7, title: {display: true, text: 'Feature Y'} }
                },
                animation: {
                    duration: 500
                },
                plugins: {
                    legend: { display: false },
                    tooltip: { enabled: false },
                    customKmeansBoundary: {
                        centroids: kmeansCentroids // Pass centroids for boundary drawing
                    }
                }
            },
            plugins: [{
                id: 'customKmeansBoundary',
                afterDraw: (chart, args, options) => {
                    const ctx = chart.ctx;
                    const xAxis = chart.scales.x;
                    const yAxis = chart.scales.y;

                    if (options.centroids.length === 2) {
                        const c1 = options.centroids[0];
                        const c2 = options.centroids[1];

                        // Calculate midpoint
                        const midX = (c1.x + c2.x) / 2;
                        const midY = (c1.y + c2.y) / 2;

                        // Calculate slope of line connecting centroids
                        const slope = (c2.y - c1.y) / (c2.x - c1.x);
                        let perpSlope;
                        if (slope === 0) { // Horizontal line, vertical bisector
                            perpSlope = Infinity;
                        } else if (isFinite(slope)) {
                            perpSlope = -1 / slope; // Perpendicular slope
                        } else { // Vertical line, horizontal bisector
                            perpSlope = 0;
                        }

                        ctx.strokeStyle = 'rgba(128, 0, 128, 0.7)'; // Purple line
                        ctx.lineWidth = 2;
                        ctx.setLineDash([5, 5]); // Dashed line

                        ctx.beginPath();
                        if (!isFinite(perpSlope)) { // Vertical line
                            const pixelMidX = xAxis.getPixelForValue(midX);
                            ctx.moveTo(pixelMidX, yAxis.top);
                            ctx.lineTo(pixelMidX, yAxis.bottom);
                        } else if (perpSlope === 0) { // Horizontal line
                            const pixelMidY = yAxis.getPixelForValue(midY);
                            ctx.moveTo(xAxis.left, pixelMidY);
                            ctx.lineTo(xAxis.right, pixelMidY);
                        } else {
                            // y - midY = perpSlope * (x - midX)
                            // y = perpSlope * x - perpSlope * midX + midY
                            const y1 = perpSlope * xAxis.min + (-perpSlope * midX + midY);
                            const y2 = perpSlope * xAxis.max + (-perpSlope * midX + midY);

                            const pixelX1 = xAxis.getPixelForValue(xAxis.min);
                            const pixelY1 = yAxis.getPixelForValue(y1);
                            const pixelX2 = xAxis.getPixelForValue(xAxis.max);
                            const pixelY2 = yAxis.getPixelForValue(y2);

                            ctx.moveTo(pixelX1, pixelY1);
                            ctx.lineTo(pixelX2, pixelY2);
                        }
                        ctx.stroke();
                        ctx.setLineDash([]); // Reset line dash
                    }
                }
            }]
        });
    }

    function runKmeansStep() {
        if (kmeansIteration >= maxKmeansIterations) {
            document.getElementById('kmeansIterationText').textContent = `Converged (Iteration: ${kmeansIteration})`;
            return;
        }

        kmeansIteration++;
        document.getElementById('kmeansIterationText').textContent = `Iteration: ${kmeansIteration}`;

        // E-Step: Assign points to closest centroid (hard assignment)
        initialKmeansPoints.forEach(p => {
            let minDist = Infinity;
            let closestCluster = -1;
            kmeansCentroids.forEach((c, kIndex) => {
                const dist = Math.sqrt((p.x - c.x)**2 + (p.y - c.y)**2);
                if (dist < minDist) {
                    minDist = dist;
                    closestCluster = kIndex;
                }
            });
            p.cluster = closestCluster;
            p.color = closestCluster === 0 ? 'rgba(59, 130, 246, 0.7)' : 'rgba(220, 38, 38, 0.7)'; // Blue or Red
        });

        // M-Step: Update centroids
        kmeansCentroids.forEach((c, kIndex) => {
            let sumX = 0;
            let sumY = 0;
            let count = 0;
            initialKmeansPoints.forEach(p => {
                if (p.cluster === kIndex) {
                    sumX += p.x;
                    sumY += p.y;
                    count++;
                }
            });
            if (count > 0) {
                c.x = sumX / count;
                c.y = sumY / count;
            }
        });
        
        createKmeansChart();
    }

    document.getElementById('kmeansNextStep').addEventListener('click', runKmeansStep);
    document.getElementById('kmeansReset').addEventListener('click', () => {
        kmeansIteration = 0;
        document.getElementById('kmeansIterationText').textContent = `Iteration: 0`;
        initialKmeansPoints.forEach((p, i) => { // Reset points to initial positions and colors
            p.x = initialKmeansPoints[i].x;
            p.y = initialKmeansPoints[i].y;
            p.cluster = -1;
            p.color = 'rgba(128, 128, 128, 0.5)';
        });
        kmeansCentroids = [ // Reset positions
            { x: 1.0, y: 6.0, color: 'rgba(59, 130, 246, 1)' },
            { x: 6.0, y: 2.0, color: 'rgba(220, 38, 38, 1)' }
        ];
        createKmeansChart();
    });

    // Initial chart creations
    createEmChart();
    createKmeansChart();
    
</script>
</body>
</html>
